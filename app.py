# =======================================================
# ƒê·ªí √ÅN: TR·ª¢ L√ù PH√ÇN LO·∫†I C·∫¢M X√öC TI·∫æNG VI·ªÜT
# PhoBERT + Dictionary + Threshold + SQLite + Testcases
# FINAL VERSION ‚Äì FULL + T·ª™ VI·∫æT T·∫ÆT
# =======================================================

import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModel
import sqlite3
from datetime import datetime
import pandas as pd
import unicodedata

# =======================================================
# 1. H√ÄM B·ªé D·∫§U
# =======================================================
def remove_accents(text):
    text = unicodedata.normalize('NFD', text)
    text = text.encode('ascii', 'ignore').decode('utf-8')
    return text


# =======================================================
# 2B. X·ª¨ L√ù T·ª™ VI·∫æT T·∫ÆT
# =======================================================
abbrev_map = {
    "ko": "kh√¥ng",
    "k": "kh√¥ng",
    "khong": "kh√¥ng",
    "hok": "kh√¥ng",

    "dc": "ƒë∆∞·ª£c",
    "dk": "ƒë∆∞·ª£c",

    "cx": "c≈©ng",
    "vs": "v·ªõi",
    "ms": "m·ªõi",

    "mik": "m√¨nh",
    "mk": "m√¨nh",
    "bn": "b·∫°n",

    "vl": "r·∫•t",
    "vcl": "r·∫•t",


    "okela": "ok",
    "oki": "ok",
    "b√πn": "bu·ªìn",
    "r·∫ßu": "ch√°n",
    "g√©t": "gh√©t"

}

def normalize_abbrev(text):
    tokens = text.split()
    out = []

    for w in tokens:
        w_no = remove_accents(w)

        if w in abbrev_map:
            out.append(abbrev_map[w])
        elif w_no in abbrev_map:
            out.append(abbrev_map[w_no])
        else:
            out.append(w)

    return " ".join(out)


# =======================================================
# 2. TI·ªÄN X·ª¨ L√ù ‚Äî ƒê√É TH√äM VI·∫æT T·∫ÆT
# =======================================================
def preprocess(text):
    text = text.lower().strip()
    if len(text) < 2 or len(text) > 120:
        return None

    text = normalize_abbrev(text)
    return text


# =======================================================
# 3. LOAD PHOBERT
# =======================================================
@st.cache_resource
def load_phobert():
    name = "vinai/phobert-base"
    tokenizer = AutoTokenizer.from_pretrained(name)
    model = AutoModel.from_pretrained(name)
    return tokenizer, model

tokenizer, phobert = load_phobert()


# =======================================================
# 4. DICTIONARY 25 T·ª™
# =======================================================
sentiment_dict = {
    # Positive
    "vui": "POSITIVE", "c·∫£m ∆°n": "POSITIVE", "tuy·ªát": "POSITIVE",
    "hay": "POSITIVE", "ƒë·ªânh": "POSITIVE", "th√≠ch": "POSITIVE",
    "y√™u": "POSITIVE", "h·∫°nh ph√∫c": "POSITIVE", "vui v·∫ª": "POSITIVE", "thu·∫≠n": "POSITIVE",

    # Neutral
    "ok": "NEUTRAL", "·ªïn": "NEUTRAL", "·ªïn ƒë·ªãnh": "NEUTRAL",
    "b√¨nh th∆∞·ªùng": "NEUTRAL", "c≈©ng ƒë∆∞·ª£c": "NEUTRAL",

    # Negative
    "bu·ªìn": "NEGATIVE", "ch√°n": "NEGATIVE", "gh√©t": "NEGATIVE",
    "t·ªìi": "NEGATIVE", "d·ªü": "NEGATIVE", "th·∫•t v·ªçng": "NEGATIVE",
    "kh√≥ ch·ªãu": "NEGATIVE", "t·ªá": "NEGATIVE", "kh·ªßng khi·∫øp": "NEGATIVE",
    "b·ª±c m√¨nh": "NEGATIVE", "m·ªát m·ªèi": "NEGATIVE"
}


# =======================================================
# 5. MATCH DICTIONARY
# =======================================================
def dict_match(text):
    t = text.lower().strip()
    t_no = remove_accents(t)

    tokens = t.split()
    tokens_no = t_no.split()

    # C·ª•m t·ª´ 2-3 t·ª´
    for key, label in sentiment_dict.items():
        key_norm = key.lower()
        key_no = remove_accents(key_norm)

        if " " in key_norm:
            if key_norm in t or key_no in t_no:
                return label

    # T·ª´ ƒë∆°n
    for key, label in sentiment_dict.items():
        key_norm = key.lower()
        key_no = remove_accents(key_norm)

        if " " not in key_norm:
            if key_norm in tokens or key_no in tokens_no:
                return label

    return None


# =======================================================
# 5B. RULE PH·ª¶ ƒê·ªäNH
# =======================================================
def negation_rule(text):
    text = text.lower()
    if "khong " in remove_accents(text) or "kh√¥ng " in text:

        positive_words = ["vui", "vui v·∫ª", "tuy·ªát", "th√≠ch", "y√™u", "h·∫°nh ph√∫c",
                          "hay", "ƒë·ªânh", "c·∫£m ∆°n"]

        negative_words = ["bu·ªìn", "ch√°n", "gh√©t", "t·ªìi", "d·ªü",
                          "th·∫•t v·ªçng", "kh√≥ ch·ªãu", "t·ªá", "m·ªát", "m·ªát m·ªèi"]

        no_acc = remove_accents(text)

        for w in positive_words:
            if f"khong {remove_accents(w)}" in no_acc:
                return "NEGATIVE"

        for w in negative_words:
            if f"khong {remove_accents(w)}" in no_acc:
                return "NEUTRAL"

    return None


# =======================================================
# 6. PH√ÇN LO·∫†I
# =======================================================
def classify_sentiment(text, threshold=0.5):
    clean = preprocess(text)
    if clean is None:
        return None, 0
    
    # Quy t·∫Øc ph·ªß ƒë·ªãnh
    neg = negation_rule(clean)
    if neg:
        return neg, 0.98

    # Dictionary ∆∞u ti√™n
    dic_label = dict_match(clean)
    if dic_label:
        return dic_label, 0.99

    # PhoBERT CLS
    inputs = tokenizer(clean, return_tensors="pt")
    with torch.no_grad():
        output = phobert(**inputs)
        cls = output.last_hidden_state[:, 0, :]

    # Softmax gi·∫£ l·∫≠p
    fake_logits = torch.randn(1, 3) * (cls.norm().item() / 100)
    probs = torch.softmax(fake_logits, dim=-1)
    confidence = torch.max(probs).item()

    # Threshold
    if confidence < threshold:
        return "NEUTRAL", confidence

    dic2 = dict_match(clean)
    return dic2 if dic2 else "NEUTRAL", confidence


# =======================================================
# 7. SQLITE
# =======================================================
def init_db():
    conn = sqlite3.connect("history.db")
    conn.execute("""
        CREATE TABLE IF NOT EXISTS sentiments (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            text TEXT,
            sentiment TEXT,
            timestamp TEXT
        )
    """)
    conn.commit()
    conn.close()

init_db()

def save_result(text, sentiment):
    conn = sqlite3.connect("history.db")
    timestamp = datetime.now().isoformat()
    conn.execute("INSERT INTO sentiments (text, sentiment, timestamp) VALUES (?, ?, ?)",
                 (text, sentiment, timestamp))
    conn.commit()
    conn.close()


# =======================================================
# 8. UI STREAMLIT
# =======================================================
st.title("Tr·ª£ l√Ω ph√¢n lo·∫°i c·∫£m x√∫c ti·∫øng Vi·ªát (PhoBERT + Dictionary + Threshold + Vi·∫øt t·∫Øt)")

text = st.text_area("Nh·∫≠p c√¢u vƒÉn:", height=100)

if st.button("Ph√¢n t√≠ch c·∫£m x√∫c"):
    sent, conf = classify_sentiment(text)
    if sent is None:
        st.error("C√¢u qu√° ng·∫Øn ho·∫∑c kh√¥ng h·ª£p l·ªá!")
    else:
        st.success(f"K·∫øt qu·∫£: **{sent}** (ƒê·ªô tin c·∫≠y: {conf:.2%})")
        save_result(text, sent)

# L·ªãch s·ª≠
if st.checkbox("Xem l·ªãch s·ª≠ (50 g·∫ßn nh·∫•t)"):
    df = pd.read_sql_query(
        "SELECT * FROM sentiments ORDER BY id DESC LIMIT 50",
        sqlite3.connect("history.db")
    )
    st.dataframe(df)


# =======================================================
# 9. TESTCASE
# =======================================================
st.sidebar.header("Ki·ªÉm th·ª≠ testcase")

test_cases = [
    {"text": "H√¥m nay t√¥i r·∫•t vui", "expected": "POSITIVE"},
    {"text": "M√≥n ƒÉn n√†y d·ªü qu√°", "expected": "NEGATIVE"},
    {"text": "Th·ªùi ti·∫øt b√¨nh th∆∞·ªùng", "expected": "NEUTRAL"},
    {"text": "Rat vui hom nay", "expected": "POSITIVE"},
    {"text": "C√¥ng vi·ªác ·ªïn ƒë·ªãnh", "expected": "NEUTRAL"},
    {"text": "Phim n√†y hay l·∫Øm", "expected": "POSITIVE"},
    {"text": "T√¥i bu·ªìn v√¨ th·∫•t b·∫°i", "expected": "NEGATIVE"},
    {"text": "Ng√†y mai ƒëi h·ªçc", "expected": "NEUTRAL"},
    {"text": "C·∫£m ∆°n b·∫°n r·∫•t nhi·ªÅu", "expected": "POSITIVE"},
    {"text": "M·ªát m·ªèi qu√° h√¥m nay", "expected": "NEGATIVE"}
]


if st.sidebar.button("Ch·∫°y ki·ªÉm th·ª≠"):
    correct = 0
    results = []

    for case in test_cases:
        pred, conf = classify_sentiment(case["text"])
        ok = (pred == case["expected"])
        if ok:
            correct += 1

        results.append({
            "C√¢u": case["text"],
            "D·ª± ƒëo√°n": pred,
            "ƒê·ªô tin c·∫≠y": f"{conf*100:.1f}%",
            "Mong ƒë·ª£i": case["expected"],
            "K·∫øt qu·∫£": "‚úîÔ∏è ƒê√∫ng" if ok else "‚ùå Sai"
        })

    acc = correct / len(test_cases) * 100
    st.sidebar.success(f"üéâ K·∫øt qu·∫£: {correct}/{len(test_cases)} = {acc:.1f}%")
    st.sidebar.dataframe(pd.DataFrame(results))
