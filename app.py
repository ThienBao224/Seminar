import streamlit as st
import torch
from transformers import pipeline
import sqlite3
from datetime import datetime
import pandas as pd
import unicodedata
import re

# C·∫§U H√åNH TRANG (PH·∫¢I ·ªû ƒê·∫¶U TI√äN!)
st.set_page_config(
    page_title="Ph√¢n lo·∫°i c·∫£m x√∫c",
    page_icon="ü§ñ",
    layout="wide"
)

# =======================================================
# 1. H√ÄM B·ªé D·∫§U
# =======================================================
def remove_accents(text):
    text = unicodedata.normalize('NFD', text)
    text = text.encode('ascii', 'ignore').decode('utf-8')
    return text

# =======================================================
# 2. X·ª¨ L√ù VI·∫æT T·∫ÆT
# =======================================================
abbrev_map = {
    "ko": "kh√¥ng", "k": "kh√¥ng", "khong": "kh√¥ng", "hok": "kh√¥ng",
    "dc": "ƒë∆∞·ª£c", "dk": "ƒë∆∞·ª£c", "ƒëc": "ƒë∆∞·ª£c",
    "cx": "c≈©ng", "vs": "v·ªõi", "ms": "m·ªõi",
    "mik": "m√¨nh", "mk": "m√¨nh", "bn": "b·∫°n",
    "vl": "r·∫•t", "vcl": "r·∫•t", "rat": "r·∫•t", "r√°t": "r·∫•t","tuyet": "tuy·ªát",
    "okela": "ok", "oki": "ok", "okii": "ok",
    "b√πn": "bu·ªìn", "bun": "bu·ªìn", "zui": "vui", "dui": "vui", "hihi": "vui", 
    "r·∫ßu": "bu·ªìn", "g√©t": "gh√©t", "met": "m·ªát", "moi": "m·ªèi",
    "qua": "qu√°", "wa": "qu√°", "z": "v·∫≠y", "v": "v·∫≠y",
    "ntn": "nh∆∞ th·∫ø n√†o", "the": "th·∫ø", "bik": "bi·∫øt", "bit": "bi·∫øt",
    "do": "d·ªü", "on": "·ªïn", "dinh": "ƒë·ªãnh", "lam": "l·∫Øm","la": "l√†",
    "nay": "n√†y", "hom": "h√¥m", "toi": "t√¥i", "vi": "v√¨",
    "that": "th·∫•t", "bai": "b·∫°i", "ngay": "ng√†y", "mai": "mai",
    "di": "ƒëi", "cam": "c·∫£m", "nhieu": "nhi·ªÅu",
    "thoi": "th·ªùi", "tiet": "ti·∫øt", "binh": "b√¨nh", "thuong": "th∆∞·ªùng",
    "cong": "c√¥ng", "viec": "vi·ªác", "mon": "m√≥n", "an": "ƒÉn"
}

def normalize_abbrev(text):
    tokens = text.split()
    out = []
    for w in tokens:
        w_no = remove_accents(w)
        if w in abbrev_map:
            out.append(abbrev_map[w])
        elif w_no in abbrev_map:
            out.append(abbrev_map[w_no])
        else:
            out.append(w)
    return " ".join(out)

# =======================================================
# 3. TI·ªÄN X·ª¨ L√ù
# =======================================================
def preprocess(text):
    text = text.lower().strip()
    if len(text) < 5 or len(text) > 50:
        return None
    return normalize_abbrev(text)

# 4. LOAD PHOBERT (TH·ª¨ NHI·ªÄU MODEL)
@st.cache_resource
def load_pipeline():
    try:
        # Model 1: PhoBERT fine-tuned t·ªët nh·∫•t
        model_name = "uitnlp/visobert"
        return pipeline("sentiment-analysis", model=model_name, tokenizer=model_name), "ViSoBERT"
    except:
        try:
            # Model 2: Fallback
            model_name = "wonrax/phobert-base-vietnamese-sentiment"
            return pipeline("sentiment-analysis", model=model_name, tokenizer=model_name), "PhoBERT"
        except:
            # Model 3: Universal
            model_name = "lxyuan/distilbert-base-multilingual-cased-sentiments-student"
            return pipeline("sentiment-analysis", model=model_name, tokenizer=model_name), "DistilBERT"

classifier, model_name = load_pipeline()

# =======================================================
# 5. DICTIONARY
# =======================================================
sentiment_dict = {
    # POSITIVE - m·ªü r·ªông
    "vui": "POSITIVE", "vui v·∫ª": "POSITIVE", "r·∫•t vui": "POSITIVE",
    "c·∫£m ∆°n": "POSITIVE", "tuy·ªát": "POSITIVE", "tuy·ªát v·ªùi": "POSITIVE",
    "hay": "POSITIVE", "hay l·∫Øm": "POSITIVE", "ƒë·ªânh": "POSITIVE",
    "th√≠ch": "POSITIVE", "y√™u": "POSITIVE", "h·∫°nh ph√∫c": "POSITIVE",
    "ok": "POSITIVE", "·ªïn": "POSITIVE", "t·ªët": "POSITIVE",
    "xu·∫•t s·∫Øc": "POSITIVE", "ho√†n h·∫£o": "POSITIVE", "tuy·ªát v·ªùi": "POSITIVE",
    
    # NEUTRAL - m·ªü r·ªông
    "·ªïn ƒë·ªãnh": "NEUTRAL", "b√¨nh th∆∞·ªùng": "NEUTRAL", "c≈©ng ƒë∆∞·ª£c": "NEUTRAL",
    "th·ªùi ti·∫øt": "NEUTRAL", "ƒëi h·ªçc": "NEUTRAL", "ng√†y mai": "NEUTRAL",
    "c√¥ng vi·ªác": "NEUTRAL", "h·ªçc h√†nh": "NEUTRAL","t·∫°m ƒë∆∞·ª£c ": "NEUTRAL",
    
    # NEGATIVE - m·ªü r·ªông
    "bu·ªìn": "NEGATIVE", "bu·ªìn v√¨": "NEGATIVE", "ch√°n": "NEGATIVE","tuy·ªát v·ªçng": "NEGATIVE", "buon ": "NEGATIVE",
    "gh√©t": "NEGATIVE", "t·ªìi": "NEGATIVE", "d·ªü": "NEGATIVE", "d·ªü qu√°": "NEGATIVE",
    "th·∫•t v·ªçng": "NEGATIVE", "th·∫•t b·∫°i": "NEGATIVE", "kh√≥ ch·ªãu": "NEGATIVE",
    "t·ªá": "NEGATIVE", "kh·ªßng khi·∫øp": "NEGATIVE", "b·ª±c m√¨nh": "NEGATIVE",
    "m·ªát m·ªèi": "NEGATIVE", "m·ªát m·ªèi qu√°": "NEGATIVE", "t·ªá qu√°": "NEGATIVE"
}

# ACCENT DICTIONARY (KH√îI PH·ª§C D·∫§U ƒê·ªÇ HI·ªÇN TH·ªä)
accent_dict = {
    # ƒë·∫°i t·ª´ ‚Äì c∆° b·∫£n
    "toi": "t√¥i",
    "minh": "m√¨nh",
    "ban": "b·∫°n",

    # th·ªùi gian
    "hom nay": "h√¥m nay",
    "ngay mai": "ng√†y mai",
    "bay gio": "b√¢y gi·ªù",
    "di qua": "ƒëi qua",

    # c·∫£m x√∫c t√≠ch c·ª±c
    "rat vui": "r·∫•t vui",
    "vui": "vui",
    "hanh phuc": "h·∫°nh ph√∫c",
    "yeu": "y√™u",
    "thich": "th√≠ch",
    "tuyet voi": "tuy·ªát v·ªùi",
    "cam on": "c·∫£m ∆°n",

    # c·∫£m x√∫c ti√™u c·ª±c
    "buon": "bu·ªìn",
    "chan": "ch√°n",
    "that vong": "th·∫•t v·ªçng",
    "tuyet vong": "tuy·ªát v·ªçng",
    "met moi": "m·ªát m·ªèi",
    "te": "t·ªá",
    "do qua": "d·ªü qu√°",

    # trung t√≠nh
    "binh thuong": "b√¨nh th∆∞·ªùng",
    "cong viec": "c√¥ng vi·ªác",
    " thay": " th·∫•y",
    "thoi tiet": "th·ªùi ti·∫øt"
}

# =======================================================
# 6. MATCH DICTIONARY 
# =======================================================
def dict_match(text):
    t = text.lower().strip()
    t_no = remove_accents(t)

    words = t_no.split()

    # ∆Øu ti√™n c·ª•m t·ª´ d√†i
    sorted_keys = sorted(sentiment_dict.keys(), key=lambda x: -len(x.split()))

    for key in sorted_keys:
        key_no = remove_accents(key.lower())
        key_words = key_no.split()

        # So kh·ªõp c·ª•m t·ª´ theo word boundary
        if len(key_words) > 1:
            if key_no in t_no:
                return sentiment_dict[key]
        else:
            if key_no in words:
                return sentiment_dict[key]

    return None


# KH√îI PH·ª§C D·∫§U TI·∫æNG VI·ªÜT (CH·ªà ƒê·ªÇ HI·ªÇN TH·ªä)
def restore_accents(text):
    text = normalize_abbrev(text.lower())
    text_no = remove_accents(text)
    result = text

    sorted_keys = sorted(accent_dict.keys(), key=lambda x: -len(x.split()))

    for key in sorted_keys:
        key_no = remove_accents(key)
        # ch·ªâ thay th·∫ø t·ª´ nguy√™n v·∫πn (whole word)
        result = re.sub(r'\b' + re.escape(key_no) + r'\b', accent_dict[key], result)
    
    return result


# =======================================================
# 7. RULE PH·ª¶ ƒê·ªäNH 
# =======================================================
def negation_rule(text):
    text_low = text.lower()
    no_acc = remove_accents(text_low)
    
    negation_words = ["khong", "kh√¥ng", "ch∆∞a", "ch·∫£"]
    
    for neg in negation_words:
        if neg in no_acc or neg in text_low:
            positive_words = ["vui", "tuy·ªát", "th√≠ch", "y√™u", "h·∫°nh ph√∫c", 
                            "hay", "ƒë·ªânh", "t·ªët", "ok", "·ªïn"]
            negative_words = ["bu·ªìn", "ch√°n", "gh√©t", "t·ªìi", "d·ªü",
                            "th·∫•t v·ªçng", "t·ªá", "m·ªát"]
            
            for w in positive_words:
                if f"{neg} {remove_accents(w)}" in no_acc:
                    return "NEGATIVE"
            
            for w in negative_words:
                if f"{neg} {remove_accents(w)}" in no_acc:
                    return "NEUTRAL"
    
    return None

# =======================================================
# 8. CHU·∫®N H√ìA NH√ÉN (TI·∫æNG ANH ‚Üí TI·∫æNG VI·ªÜT)
# =======================================================
def normalize_label(label):
    label_upper = label.upper()
    
    label_map = {
        # Ti·∫øng Anh
        "POS": "POSITIVE", "NEG": "NEGATIVE", "NEU": "NEUTRAL",
        "POSITIVE": "POSITIVE", "NEGATIVE": "NEGATIVE", "NEUTRAL": "NEUTRAL",
        # Label s·ªë
        "LABEL_0": "NEGATIVE", "LABEL_1": "NEUTRAL", "LABEL_2": "POSITIVE",
        "0": "NEGATIVE", "1": "NEUTRAL", "2": "POSITIVE",
    }
    
    return label_map.get(label_upper, "NEUTRAL")

def label_to_vietnamese(label):
    """Chuy·ªÉn nh√£n sang ti·∫øng Vi·ªát"""
    vn_map = {
        "POSITIVE": "T√≠ch c·ª±c",
        "NEGATIVE": "Ti√™u c·ª±c",
        "NEUTRAL": "Trung t√≠nh"
    }
    return vn_map.get(label, label)

def get_emoji(label):
    """L·∫•y emoji theo nh√£n"""
    emoji_map = {
        "POSITIVE": "üòä",
        "NEGATIVE": "üòû",
        "NEUTRAL": "üòê"
    }
    return emoji_map.get(label, "‚ùì")

# =======================================================
# 9. PH√ÇN LO·∫†I SENTIMENT 
# =======================================================
def classify_sentiment(text, threshold=0.55):  
    clean = preprocess(text)
    if clean is None:
        return None, 0.0

    # 1. Rule ph·ªß ƒë·ªãnh (∆∞u ti√™n cao nh·∫•t)
    neg_label = negation_rule(clean)
    if neg_label:
        return normalize_label(neg_label), 0.92

    try:
        # 2. Ch·∫°y model tr∆∞·ªõc
        result = classifier(clean)[0]
        label = normalize_label(result['label'])
        confidence = result['score']

        # 3. ∆ØU TI√äN T·ª™ TI√äU C·ª∞C TRONG DICT (CH·ªêNG NGU MODEL)
        dic_label = dict_match(clean)
        if dic_label == "NEGATIVE":
            return "NEGATIVE", 0.95

        # 4. N·∫øu model t·ª± tin ‚Üí tin model
        if confidence >= threshold:
            return label, confidence

        # 5. N·∫øu model kh√¥ng t·ª± tin ‚Üí fallback dictionary
        if dic_label:
            return normalize_label(dic_label), min(confidence + 0.15, 0.85)

        # 6. Fallback t√°ch token
        tokens = clean.split()
        for token in tokens:
            token_label = dict_match(token)
            if token_label:
                return normalize_label(token_label), 0.68

        # 7. Cu·ªëi c√πng fallback neutral
        return "NEUTRAL", confidence

    except:
        # 8. N·∫øu model l·ªói ‚Üí d√πng dictionary
        dic_label = dict_match(clean)
        if dic_label:
            return normalize_label(dic_label), 0.75
        return "NEUTRAL", 0.5

# =======================================================
# 10. SQLITE
# =======================================================
def init_db():
    conn = sqlite3.connect("history.db")
    conn.execute("""
        CREATE TABLE IF NOT EXISTS sentiments (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            text TEXT,
            sentiment TEXT,
            timestamp TEXT
        )
    """)
    conn.commit()
    conn.close()

def save_result(original_text, sentiment):
    display_text = restore_accents(original_text)
    conn = sqlite3.connect("history.db")
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    conn.execute(
        "INSERT INTO sentiments (text, sentiment, timestamp) VALUES (?, ?, ?)",
        (display_text, sentiment, timestamp)
    )
    conn.commit()
    conn.close()


init_db()


# =======================================================
# 11. TESTCASE
# =======================================================


test_cases = [
    {"text": "H√¥m nay t√¥i r·∫•t vui", "expected": "POSITIVE"},
    {"text": "M√≥n ƒÉn n√†y d·ªü qu√°", "expected": "NEGATIVE"},
    {"text": "Th·ªùi ti·∫øt b√¨nh th∆∞·ªùng", "expected": "NEUTRAL"},
    {"text": "R·∫•t vui h√¥m nay", "expected": "POSITIVE"},
    {"text": "C√¥ng vi·ªác ·ªïn ƒë·ªãnh", "expected": "NEUTRAL"},
    {"text": "Phim n√†y hay l·∫Øm", "expected": "POSITIVE"},
    {"text": "T√¥i bu·ªìn v√¨ th·∫•t b·∫°i", "expected": "NEGATIVE"},
    {"text": "Ng√†y mai ƒëi h·ªçc", "expected": "NEUTRAL"},
    {"text": "C·∫£m ∆°n b·∫°n r·∫•t nhi·ªÅu", "expected": "POSITIVE"},
    {"text": "M·ªát m·ªèi qu√° h√¥m nay", "expected": "NEGATIVE"},
]

# =======================================================
# 12. STREAMLIT UI - GIAO DI·ªÜN DASHBOARD 
# =======================================================

# ---------------- SIDEBAR ----------------
st.sidebar.title("Ch·ªçn ch·ª©c nƒÉng")

page = st.sidebar.radio(
    "Ch·ªçn ph·∫ßn hi·ªÉn th·ªã:",   
    ["Ph√¢n lo·∫°i c·∫£m x√∫c", "Xem l·ªãch s·ª≠", "B·ªô Testcase"]
)

st.sidebar.markdown("---")
st.sidebar.markdown(" Tr·ª£ l√Ω ph√¢n lo·∫°i c·∫£m x√∫c ti·∫øng Vi·ªát")

# PAGE 1 - PH√ÇN LO·∫†I
if page == "Ph√¢n lo·∫°i c·∫£m x√∫c":

    st.title("X√ÇY D·ª∞NG TR·ª¢ L√ù PH√ÇN LO·∫†I C·∫¢M X√öC TI·∫æNG VI·ªÜT S·ª¨ D·ª§NG TRANSFORMER")
    st.caption("ƒê·ªÄ T√ÄI SEMINAR")

    st.info(f"‚úÖ ƒêang s·ª≠ d·ª•ng model: **{model_name}**")

    st.markdown("### üí¨ Nh·∫≠p vƒÉn b·∫£n c·∫ßn ph√¢n t√≠ch")
    text = st.text_area("", height=150, placeholder="V√≠ d·ª•: H√¥m nay t√¥i r·∫•t vui")

    if st.button("Ph√¢n t√≠ch c·∫£m x√∫c"):
        if text.strip() == "":
            st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p n·ªôi dung")
        else:
            sent, conf = classify_sentiment(text)

            vn_label = label_to_vietnamese(sent)
            emoji = get_emoji(sent)

            st.success(f"{emoji} K·∫øt qu·∫£: {vn_label} ({conf*100:.1f}%)")

            st.json({
                "text_goc": text,
                "text_hien_thi": restore_accents(text),
                "sentiment": sent
            })

            save_result(text, sent)


# PAGE 2 - L·ªäCH S·ª¨
elif page == "Xem l·ªãch s·ª≠":

    st.title("üìã L·ªãch s·ª≠ ph√¢n lo·∫°i")

    conn = sqlite3.connect("history.db")
    df = pd.read_sql_query(
        "SELECT text, sentiment, timestamp FROM sentiments ORDER BY id DESC LIMIT 100",
        conn
    )
    conn.close()

    if not df.empty:
        df["C·∫£m x√∫c"] = df["sentiment"].apply(label_to_vietnamese)
        df["Icon"] = df["sentiment"].apply(get_emoji)

        st.dataframe(df[["Icon", "text", "C·∫£m x√∫c", "timestamp"]], use_container_width=True)
    else:
        st.info("Ch∆∞a c√≥ d·ªØ li·ªáu")

# PAGE 3 - TESTCASE
elif page == "B·ªô Testcase":

    st.title("üß™ B·ªô Testcase t·ª± ƒë·ªông")

    if st.button("‚ñ∂Ô∏è Ch·∫°y test t·∫•t c·∫£ c√¢u"):
        st.info("ƒêang ch·∫°y test...")

        correct = 0
        results = []

        progress_bar = st.progress(0)

        for i, case in enumerate(test_cases):
            pred, conf = classify_sentiment(case["text"])

            pred_norm = normalize_label(pred)
            expected_norm = normalize_label(case["expected"])
            ok = (pred_norm == expected_norm)

            if ok:
                correct += 1

            results.append({
                "STT": i + 1,
                "C√¢u": case["text"],
                "Mong ƒë·ª£i": label_to_vietnamese(expected_norm),
                "D·ª± ƒëo√°n": label_to_vietnamese(pred_norm),
                "ƒê·ªô tin c·∫≠y": f"{conf*100:.1f}%",
                "K·∫øt qu·∫£": "‚úÖ ƒê√∫ng" if ok else "‚ùå Sai"
            })

            progress_bar.progress((i + 1) / len(test_cases))

        progress_bar.empty()

        acc = correct / len(test_cases) * 100

        st.markdown("### üìä K·∫øt qu·∫£ ƒë√°nh gi√°")

        c1, c2, c3 = st.columns(3)
        c1.metric("ƒê√∫ng", f"{correct}/{len(test_cases)}")
        c2.metric("ƒê·ªô ch√≠nh x√°c", f"{acc:.1f}%")
        c3.metric("ƒê√°nh gi√°", "‚úÖ ƒê·∫†T" if acc >= 65 else "‚ùå CH∆ØA ƒê·∫†T")

        st.dataframe(pd.DataFrame(results), use_container_width=True)

        if acc >= 65:
            st.success(f"üéâ ƒê·∫†T y√™u c·∫ßu ({acc:.1f}%)")
        else:
            st.warning(f"‚ö†Ô∏è Ch∆∞a ƒë·∫°t y√™u c·∫ßu ({acc:.1f}%)")
